{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in previous lectures, first we import the packages necessary fo this instruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the data set \"df\" with feature variables \"x\" and \"y\" given below.\n",
    "\n",
    "Pandas.DataFrame documentation excerpt: \n",
    "*Two-dimensional, tabular data structure with labeled axes (rows and columns). Can be thought of as a dict-like container for Series objects.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'x': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'y': [1, 3, 2, 5, 7, 8, 8, 9, 10, 12]})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a first impression of the given data, let's have a look at its scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(df[['x']], df[['y']], color = \"y\", marker = \"o\", s = 40)\n",
    "plt.xlabel('x-axis') \n",
    "plt.ylabel('y-axis')\n",
    "plt.title('Scatter Plot Title') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already see a linear correlation between x and y. Assume the feature x to be descriptive, while y is our target feature. We want a linear function, y=ax+b, that predicts y as accurately as possible based on x. To achieve this goal we use linear regression from the sklearn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#illustrating adjustment of data format for method \"fit\"\n",
    "print(df[['y']])\n",
    "print(df['y'])\n",
    "\n",
    "#define the classifier\n",
    "classifier = LinearRegression()\n",
    "\n",
    "#train the classifier\n",
    "model = classifier.fit(df[['x']], df['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the classifier to predict y. We print the predictions as well as the coefficient and intercept of the linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the trained classifier to make prediction\n",
    "y_predict = classifier.predict(df[['x']])\n",
    "print(y_predict)\n",
    "\n",
    "#print coefficient (a in y=ax+b) and intercept (the constant, b in y=ax+b)\n",
    "print('Coefficients: \\n', classifier.coef_)\n",
    "print('Intercept: \\n', classifier.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our regression function with the scatterplot showing the original data set. Herefore, we use the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#visualize original data points\n",
    "plt.scatter(df[['x']], df[['y']], color = \"m\", marker = \"s\", s = 100) \n",
    "#visualize regression function\n",
    "plt.plot(df[['x']], y_predict, color = \"g\") \n",
    "plt.xlabel('x-axis') \n",
    "plt.ylabel('y-axis') \n",
    "plt.title('Regression Function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Now it is your turn. </span> Build a simple linear regression for the data below. Use col1 as descriptive feature and col2 as target feature. Also plot your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'col1': [770, 677, 428, 410, 371, 504, 1136, 695, 551, 550], 'col2': [54, 47, 28, 38, 29, 38, 80, 52, 45, 40]})\n",
    "\n",
    "#your turn - solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the model and its predictions is often not enough. Let's also compute their error. The sklearn.metrics package contains several error metrics such as\n",
    "\n",
    "* Mean squared error\n",
    "* Mean absolute error\n",
    "* Mean squared log error\n",
    "* Median absolute error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean squared error:\")\n",
    "print(mean_squared_error(df2['col2'], y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize our squared errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_predict, (df2['col2'] - y_predict) ** 2, color = \"blue\", s = 10,) \n",
    "plt.title(\"Squared errors\")\n",
    "## plotting line for zero error \n",
    "plt.hlines(y = 0, xmin = 0, xmax = 80, linewidth = 2) \n",
    "plt.xlabel('predicted values')\n",
    "plt.ylabel('squared error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Now it is your turn. </span> Compute the mean absolute error and visualize the absolute errors. Play around using different error metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling multiple descriptive features at once - Multiple linear regression\n",
    "In most cases, we will have more than one descriptive featureâ€Š. As an example, we use an example data set of the scikit package. The dataset describes housing prices in Boston based on several attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets ## imports datasets from scikit-learn\n",
    "data = datasets.load_boston()\n",
    "print(type(data)) #sklearn.utils.Bunch object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the housing price we will use a Multiple Linear Regression model. In Python this is very straightforward: we use the same function as for simple linear regression, where the parameter x can consist of multiple columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.data #predefined descriptive features of the sklearn bunch object\n",
    "y = data.target #predefined target feature of the sklearn bunch object\n",
    "print(type(X))\n",
    "print(type(y))\n",
    "\n",
    "classifier2 = LinearRegression()\n",
    "classifier2.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict2 = classifier2.predict(X)\n",
    "print(y_predict2[0:5]) #print first 5 predicted values\n",
    "\n",
    "print('Coefficients: \\n', classifier2.coef_)\n",
    "print('Intercept: \\n', classifier2.intercept_)\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y, y_predict2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling categorical descriptive features\n",
    "So far we always encountered numerical attributes, but data sets can also contain categorical attributes. The regression function can only handle numerical input. Therefore we need to tranform categorical data, for example using one-hot encoding as explained in the lecture: we introduce a 0/1 feature for every possible value of our categorical attribute.\n",
    "\n",
    "There are several possibilities to achieve this in python. Here we present the *get_dummies* function of pandas.\n",
    "\n",
    "After encoding the attributes we can apply our regular regression function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'A':['a','b','c'],'B':['c','b','a'] })\n",
    "print(df.head())\n",
    "#one-hot encoding example using pandas\n",
    "df_one_hot = pd.get_dummies(df)\n",
    "print(df_one_hot.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Now it is your turn. </span>  Perform linear regression using the data set given below. Don't forget to transform your categorical features. The rental price attribute represents the target feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Size':[500,550,620,630,665],'Floor':[4,7,9,5,8], 'Energy rating':['C', 'A', 'A', 'B', 'C'], 'Rental price': [320,380,400,390,385] })\n",
    "\n",
    "#Your turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting a categorical target value - Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also encounter data sets where our target value is categorical. Here we don't transform them into numeric values, but we use a logistic regression function. Luckily, sklearn provides us with a suitable function that is similar to the linear equivalent. Similar to linear regression we can compute logistic regression on a single descriptive feature as well as multiple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "df = pd.read_csv('iris.csv') #import iris data set\n",
    "print('All features: \\n', df.columns.tolist()) #get feature overview\n",
    "\n",
    "X = df[['sepal_length']] #we only use the first feature as descriptive feature\n",
    "y = df['species'] #use the species as target feature\n",
    "print('X: \\n', X.head(), '\\n')\n",
    "print('y: \\n', y.head(), '\\n')\n",
    "\n",
    "\n",
    "#defining and training the logistic regression model\n",
    "classifier3 = LogisticRegression(solver = 'liblinear', multi_class = 'ovr')\n",
    "classifier3.fit(X, y)\n",
    "\n",
    "pred = classifier3.predict(X)\n",
    "\n",
    "print('Score: \\n', classifier3.score(X, y)) #the mean accuracy on the given test data and labels\n",
    "print('Coefficients: \\n', classifier3.coef_)\n",
    "print('Intercept: \\n', classifier3.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Now it is your turn. </span>  In the example above we only used the first descriptive feature. Change the example to be able to handle all available descriptive features to predict the species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from regression models, the sklearn package also provides us with a function for training support vector machines. Looking at the example below we see that they can be trained in similar ways. We still use the iris data set for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('iris.csv')\n",
    "\n",
    "#prepare input data\n",
    "features = df.columns.tolist()\n",
    "features.remove('species')\n",
    "X = df[features]\n",
    "y = df['species']\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "classifier = SVC(C=1, kernel='linear')\n",
    "classifier.fit(X, y)\n",
    "\n",
    "pred = classifier.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in the lecture, a support vector machine is defined by its support vectors. In the sklearn package we can access them and their properties very easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Indicies of support vectors: \\n', classifier.support_, '\\n ') #indicies of support vectors\n",
    "print('Support vectors: \\n ', classifier.support_vectors_, '\\n ') #the support vectors\n",
    "print('Number of support vectors for each class: \\n ', classifier.n_support_, '\\n ') #the number of support vectors for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also directly evaluate the decision_function() method, that is, we compute the value a certain input example is mapped to by the trained SVM. Thus we get an idea about how 'far' the example is located from the speerating hyperplane. Score(X,y) calculates the mean accuracy of the classification. The classification report shows measures such as precision, recall, f1-score and support. You will learn more about these quality measures in a few lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('X: \\n', X[0:5], '\\n')\n",
    "print('Evaluating the decision function: \\n', classifier.decision_function(X)[0:5], '\\n')\n",
    "print('Accuracy: \\n', classifier.score(X,pred), '\\n')\n",
    "print('Classification Report: \\n', classification_report(y, pred), '\\n')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVC has many parameters, described in detail in the documentation (http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).\n",
    "In the lecture you learned about the concept of kernels. The SVC() method gives you the opportunity to try different kernel functions. \n",
    "Furthermore, the parameter C tells the SVM optimization how much you want to avoid misclassifying each training example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Now it is your turn. </span>  Play around with the parameter settings of the SVM using the previous iris example. Which settings lead to good classifications? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your turn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
